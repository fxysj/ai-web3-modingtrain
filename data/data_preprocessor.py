# æ•°æ®é¢„å¤„ç†å·¥å…·
# utils/data_preprocessing.py
import re
import json
import os
import html
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
import emoji
from typing import List, Dict, Any, Optional, Tuple, Callable
import pandas as pd
import numpy as np
from bs4 import BeautifulSoup
from transformers import PreTrainedTokenizer

# ç¡®ä¿nltkæ•°æ®å·²ä¸‹è½½
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('punkt')
    nltk.download('stopwords')


class Web3DataPreprocessor:
    """Web3.0é¢†åŸŸæ–‡æœ¬æ•°æ®é¢„å¤„ç†å·¥å…·"""

    def __init__(self, tokenizer: Optional[PreTrainedTokenizer] = None,
                 language: str = 'english', lower_case: bool = True,
                 remove_stopwords: bool = True, normalize_web3_terms: bool = True):
        """
        åˆå§‹åŒ–æ•°æ®é¢„å¤„ç†å™¨

        Args:
            tokenizer: å¯é€‰çš„åˆ†è¯å™¨ï¼Œç”¨äºç¼–ç æ–‡æœ¬
            language: è¯­è¨€è®¾ç½®ï¼Œå½±å“åœç”¨è¯ç­‰
            lower_case: æ˜¯å¦å°†æ–‡æœ¬è½¬æ¢ä¸ºå°å†™
            remove_stopwords: æ˜¯å¦ç§»é™¤åœç”¨è¯
            normalize_web3_terms: æ˜¯å¦æ ‡å‡†åŒ–Web3.0é¢†åŸŸæœ¯è¯­
        """
        self.tokenizer = tokenizer
        self.language = language
        self.lower_case = lower_case
        self.remove_stopwords = remove_stopwords
        self.normalize_web3_terms = normalize_web3_terms

        # åŠ è½½åœç”¨è¯
        self.stop_words = set(stopwords.words(language))

        # Web3.0é¢†åŸŸæœ¯è¯­æ˜ å°„è¡¨ï¼ˆç”¨äºæ ‡å‡†åŒ–ï¼‰
        self.web3_term_mapping = {
            "eth": "ä»¥å¤ªåŠ",
            "btc": "æ¯”ç‰¹å¸",
            "defi": "å»ä¸­å¿ƒåŒ–é‡‘è",
            "nft": "éåŒè´¨åŒ–ä»£å¸",
            "dao": "å»ä¸­å¿ƒåŒ–è‡ªæ²»ç»„ç»‡",
            "dapp": "å»ä¸­å¿ƒåŒ–åº”ç”¨",
            "gas": "Gasè´¹",
            "wallet": "é’±åŒ…",
            "blockchain": "åŒºå—é“¾",
            "smart contract": "æ™ºèƒ½åˆçº¦",
            "mining": "æŒ–çŸ¿",
            "validator": "éªŒè¯è€…",
            "staking": "è´¨æŠ¼",
            "liquidity": "æµåŠ¨æ€§",
            "oracle": "é¢„è¨€æœº",
            "bridge": "è·¨é“¾æ¡¥",
            "layer 1": "ç¬¬ä¸€å±‚",
            "layer 2": "ç¬¬äºŒå±‚",
            "protocol": "åè®®",
            "token": "ä»£å¸",
            "transaction": "äº¤æ˜“",
            "address": "åœ°å€",
            "ç§é’¥": "ç§é’¥",
            "å…¬é’¥": "å…¬é’¥",
            "hash": "å“ˆå¸Œ",
            "ç©ºæŠ•": "ç©ºæŠ•",
            "æµåŠ¨æ€§æŒ–çŸ¿": "æµåŠ¨æ€§æŒ–çŸ¿",
            "yield farming": "æ”¶ç›Šè€•ä½œ",
            "impermanent loss": "æ— å¸¸æŸå¤±"
        }

        # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        self.url_pattern = re.compile(r'https?://\S+|www\.\S+')
        self.email_pattern = re.compile(r'\S+@\S+\.\S+')
        self.mention_pattern = re.compile(r'@\w+')
        self.hashtag_pattern = re.compile(r'#\w+')
        self.eth_address_pattern = re.compile(r'0x[a-fA-F0-9]{40}')
        self.number_pattern = re.compile(r'\d+(\.\d+)?')
        self.punctuation_pattern = re.compile(f'[{re.escape(string.punctuation)}]')

        # è¡¨æƒ…ç¬¦å·æ˜ å°„ï¼ˆå°†è¡¨æƒ…ç¬¦å·è½¬æ¢ä¸ºæ–‡æœ¬æè¿°ï¼‰
        self.emoji_mapping = {
            "ğŸš€": "ä¸Šå‡",
            "ğŸ“‰": "ä¸‹é™",
            "ğŸ’": "åŠ å¯†è´§å¸",
            "ğŸ‚": "ç‰›å¸‚",
            "ğŸ»": "ç†Šå¸‚",
            "ğŸ’°": "é‡‘é’±",
            "ğŸ’¸": "æ”¯å‡º",
            "ğŸ’¹": "å›¾è¡¨ä¸Šå‡",
            "ğŸ“Š": "å›¾è¡¨",
            "ğŸ”’": "å®‰å…¨",
            "ğŸ”“": "ä¸å®‰å…¨",
            "âš ï¸": "è­¦å‘Š",
            "âœ…": "æˆåŠŸ",
            "âŒ": "å¤±è´¥"
        }

    def clean_text(self, text: str) -> str:
        """
        æ‰§è¡ŒåŸºæœ¬æ–‡æœ¬æ¸…æ´—

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            æ¸…æ´—åçš„æ–‡æœ¬
        """
        # è§£ç HTMLå®ä½“
        text = html.unescape(text)

        # ç§»é™¤HTMLæ ‡ç­¾
        text = BeautifulSoup(text, "html.parser").get_text()

        # ç§»é™¤URL
        text = self.url_pattern.sub('', text)

        # ç§»é™¤ç”µå­é‚®ä»¶
        text = self.email_pattern.sub('', text)

        # ç§»é™¤æåŠ
        text = self.mention_pattern.sub('', text)

        # ç§»é™¤ETHåœ°å€
        text = self.eth_address_pattern.sub('ETH_ADDRESS', text)

        # è½¬æ¢è¡¨æƒ…ç¬¦å·ä¸ºæ–‡æœ¬
        text = emoji.demojize(text)
        for emoji_symbol, text_desc in self.emoji_mapping.items():
            text = text.replace(emoji_symbol, f" {text_desc} ")

        # æ ‡å‡†åŒ–Web3.0æœ¯è¯­
        if self.normalize_web3_terms:
            for term, replacement in self.web3_term_mapping.items():
                # ä½¿ç”¨å•è¯è¾¹ç•Œç¡®ä¿ç²¾ç¡®åŒ¹é…
                term_pattern = re.compile(rf'\b{term}\b', re.IGNORECASE)
                text = term_pattern.sub(replacement, text)

        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text).strip()

        return text

    def normalize_text(self, text: str) -> str:
        """
        æ‰§è¡Œæ–‡æœ¬æ ‡å‡†åŒ–

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            æ ‡å‡†åŒ–åçš„æ–‡æœ¬
        """
        # è½¬æ¢ä¸ºå°å†™ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.lower_case:
            text = text.lower()

        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        text = self.punctuation_pattern.sub(' ', text)

        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text).strip()

        return text

    def tokenize_text(self, text: str) -> List[str]:
        """
        åˆ†è¯æ–‡æœ¬

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            åˆ†è¯åçš„åˆ—è¡¨
        """
        # ä½¿ç”¨NLTKåˆ†è¯å™¨
        tokens = word_tokenize(text, language=self.language)

        # ç§»é™¤åœç”¨è¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.remove_stopwords:
            tokens = [token for token in tokens if token not in self.stop_words]

        return tokens

    def encode_text(self, text: str, max_length: int = 128,
                    padding: str = 'max_length', truncation: bool = True) -> Dict[str, List[int]]:
        """
        ä½¿ç”¨åˆ†è¯å™¨ç¼–ç æ–‡æœ¬

        Args:
            text: è¾“å…¥æ–‡æœ¬
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            padding: å¡«å……æ–¹å¼
            truncation: æ˜¯å¦æˆªæ–­

        Returns:
            ç¼–ç åçš„è¾“å…¥å­—å…¸
        """
        if self.tokenizer is None:
            raise ValueError("Tokenizer is required for encoding text")

        return self.tokenizer(
            text,
            max_length=max_length,
            padding=padding,
            truncation=truncation,
            return_tensors="pt"
        )

    def extract_numeric_features(self, text: str) -> Dict[str, Any]:
        """
        ä»æ–‡æœ¬ä¸­æå–æ•°å€¼ç‰¹å¾

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            åŒ…å«æ•°å€¼ç‰¹å¾çš„å­—å…¸
        """
        features = {}

        # æå–æ‰€æœ‰æ•°å­—
        numbers = [float(num) for num in self.number_pattern.findall(text)]

        if numbers:
            features["num_count"] = len(numbers)
            features["num_sum"] = sum(numbers)
            features["num_mean"] = sum(numbers) / len(numbers)
            features["num_max"] = max(numbers)
            features["num_min"] = min(numbers)
        else:
            features["num_count"] = 0
            features["num_sum"] = 0.0
            features["num_mean"] = 0.0
            features["num_max"] = 0.0
            features["num_min"] = 0.0

        # æå–ç™¾åˆ†æ¯”
        percentages = [float(p[:-1]) for p in self.percentage_pattern.findall(text)]
        features["percentages"] = percentages

        return features

    def extract_web3_features(self, text: str) -> Dict[str, Any]:
        """
        ä»æ–‡æœ¬ä¸­æå–Web3.0ç‰¹å®šç‰¹å¾

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            åŒ…å«Web3.0ç‰¹å¾çš„å­—å…¸
        """
        features = {}

        # æ£€æŸ¥æ˜¯å¦åŒ…å«ETHåœ°å€
        features["has_eth_address"] = 1 if self.eth_address_pattern.search(text) else 0

        # è®¡ç®—Web3.0æœ¯è¯­å‡ºç°æ¬¡æ•°
        term_counts = {}
        total_terms = 0

        for term in self.web3_term_mapping.values():
            count = text.lower().count(term.lower())
            term_counts[term] = count
            total_terms += count

        features["web3_term_counts"] = term_counts
        features["web3_term_total"] = total_terms

        # æ£€æŸ¥æ˜¯å¦åŒ…å«åè®®åç§°
        protocols = ["ä»¥å¤ªåŠ", "æ¯”ç‰¹å¸", "å¸å®‰æ™ºèƒ½é“¾", "Polygon", "Avalanche", "Solana", "Cosmos"]
        features["protocols_mentioned"] = [p for p in protocols if p.lower() in text.lower()]
        features["num_protocols"] = len(features["protocols_mentioned"])

        return features

    def process_text(self, text: str, max_length: int = 128) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæ–‡æœ¬ï¼Œè¿”å›æ¸…æ´—ã€æ ‡å‡†åŒ–ã€ç¼–ç åçš„ç»“æœ

        Args:
            text: è¾“å…¥æ–‡æœ¬
            max_length: æœ€å¤§åºåˆ—é•¿åº¦

        Returns:
            å¤„ç†åçš„ç‰¹å¾å­—å…¸
        """
        processed = {}

        # æ¸…æ´—æ–‡æœ¬
        cleaned_text = self.clean_text(text)
        processed["cleaned_text"] = cleaned_text

        # æ ‡å‡†åŒ–æ–‡æœ¬
        normalized_text = self.normalize_text(cleaned_text)
        processed["normalized_text"] = normalized_text

        # åˆ†è¯
        tokens = self.tokenize_text(normalized_text)
        processed["tokens"] = tokens

        # æå–æ•°å€¼ç‰¹å¾
        numeric_features = self.extract_numeric_features(cleaned_text)
        processed.update({f"numeric_{k}": v for k, v in numeric_features.items()})

        # æå–Web3.0ç‰¹å¾
        web3_features = self.extract_web3_features(cleaned_text)
        processed.update({f"web3_{k}": v for k, v in web3_features.items()})

        # ç¼–ç æ–‡æœ¬ï¼ˆå¦‚æœæœ‰åˆ†è¯å™¨ï¼‰
        if self.tokenizer is not None:
            encoding = self.encode_text(cleaned_text, max_length=max_length)
            processed["input_ids"] = encoding["input_ids"].squeeze(0).tolist()
            processed["attention_mask"] = encoding["attention_mask"].squeeze(0).tolist()

        return processed

    def process_batch(self, texts: List[str], max_length: int = 128) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡å¤„ç†æ–‡æœ¬

        Args:
            texts: è¾“å…¥æ–‡æœ¬åˆ—è¡¨
            max_length: æœ€å¤§åºåˆ—é•¿åº¦

        Returns:
            å¤„ç†åçš„ç‰¹å¾å­—å…¸åˆ—è¡¨
        """
        return [self.process_text(text, max_length=max_length) for text in texts]


class Web3DatasetProcessor:
    """Web3.0é¢†åŸŸæ•°æ®é›†å¤„ç†å™¨"""

    def __init__(self, preprocessor: Web3DataPreprocessor):
        """
        åˆå§‹åŒ–æ•°æ®é›†å¤„ç†å™¨

        Args:
            preprocessor: æ•°æ®é¢„å¤„ç†å™¨
        """
        self.preprocessor = preprocessor

    def process_dataset(self, dataset: List[Dict[str, Any]], text_field: str = "text",
                        label_field: str = "label", max_length: int = 128) -> List[Dict[str, Any]]:
        """
        å¤„ç†æ•´ä¸ªæ•°æ®é›†

        Args:
            dataset: æ•°æ®é›†ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—å…¸
            text_field: æ–‡æœ¬å­—æ®µå
            label_field: æ ‡ç­¾å­—æ®µå
            max_length: æœ€å¤§åºåˆ—é•¿åº¦

        Returns:
            å¤„ç†åçš„æ•°æ®é›†
        """
        processed_data = []

        for item in dataset:
            # ç¡®ä¿æ•°æ®é¡¹åŒ…å«æ–‡æœ¬å­—æ®µ
            if text_field not in item:
                continue

            # å¤„ç†æ–‡æœ¬
            processed_text = self.preprocessor.process_text(item[text_field], max_length=max_length)

            # åˆ›å»ºå¤„ç†åçš„æ ·æœ¬
            processed_item = {
                **processed_text,
                "original_text": item[text_field]
            }

            # æ·»åŠ æ ‡ç­¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if label_field in item:
                processed_item["label"] = item[label_field]

            # æ·»åŠ å…¶ä»–å­—æ®µï¼ˆå¦‚æœæœ‰ï¼‰
            for key, value in item.items():
                if key not in [text_field, label_field]:
                    processed_item[key] = value

            processed_data.append(processed_item)

        return processed_data

    def process_dataframe(self, df: pd.DataFrame, text_column: str = "text",
                          label_column: str = "label", max_length: int = 128) -> pd.DataFrame:
        """
        å¤„ç†DataFrameæ ¼å¼çš„æ•°æ®é›†

        Args:
            df: è¾“å…¥DataFrame
            text_column: æ–‡æœ¬åˆ—å
            label_column: æ ‡ç­¾åˆ—å
            max_length: æœ€å¤§åºåˆ—é•¿åº¦

        Returns:
            å¤„ç†åçš„DataFrame
        """
        # å¤„ç†æ¯ä¸€è¡Œ
        processed_rows = []

        for _, row in df.iterrows():
            # å¤„ç†æ–‡æœ¬
            processed_text = self.preprocessor.process_text(row[text_column], max_length=max_length)

            # åˆ›å»ºå¤„ç†åçš„è¡Œ
            processed_row = {
                **processed_text,
                "original_text": row[text_column]
            }

            # æ·»åŠ æ ‡ç­¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if label_column in row:
                processed_row["label"] = row[label_column]

            # æ·»åŠ å…¶ä»–åˆ—
            for col in df.columns:
                if col not in [text_column, label_column]:
                    processed_row[col] = row[col]

            processed_rows.append(processed_row)

        # è½¬æ¢ä¸ºDataFrame
        processed_df = pd.DataFrame(processed_rows)

        return processed_df

    def create_torch_dataset(self, dataset: List[Dict[str, Any]],
                             text_field: str = "cleaned_text",
                             label_field: str = "label") -> torch.utils.data.Dataset:
        """
        åˆ›å»ºPyTorchæ•°æ®é›†

        Args:
            dataset: æ•°æ®é›†ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—å…¸
            text_field: æ–‡æœ¬å­—æ®µå
            label_field: æ ‡ç­¾å­—æ®µå

        Returns:
            PyTorchæ•°æ®é›†
        """
        from torch.utils.data import Dataset

        class Web3Dataset(Dataset):
            def __init__(self, data, preprocessor, text_field, label_field):
                self.data = data
                self.preprocessor = preprocessor
                self.text_field = text_field
                self.label_field = label_field

            def __len__(self):
                return len(self.data)

            def __getitem__(self, idx):
                item = self.data[idx]
                text = item[self.text_field]

                # ç¼–ç æ–‡æœ¬
                encoding = self.preprocessor.encode_text(text)

                # åˆ›å»ºæ ·æœ¬
                sample = {
                    "input_ids": encoding["input_ids"].squeeze(0),
                    "attention_mask": encoding["attention_mask"].squeeze(0)
                }

                # æ·»åŠ æ ‡ç­¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if self.label_field in item:
                    sample["labels"] = torch.tensor(item[self.label_field])

                return sample

        return Web3Dataset(dataset, self.preprocessor, text_field, label_field)


# è¾…åŠ©å‡½æ•°
def load_json_data(file_path: str) -> List[Dict[str, Any]]:
    """
    ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®

    Args:
        file_path: æ–‡ä»¶è·¯å¾„

    Returns:
        åŠ è½½çš„æ•°æ®åˆ—è¡¨
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"File not found: {file_path}")

    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    return data


def save_json_data(data: List[Dict[str, Any]], file_path: str) -> None:
    """
    å°†æ•°æ®ä¿å­˜åˆ°JSONæ–‡ä»¶

    Args:
        data: æ•°æ®åˆ—è¡¨
        file_path: æ–‡ä»¶è·¯å¾„
    """
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def split_dataset(dataset: List[Dict[str, Any]], train_ratio: float = 0.8,
                  val_ratio: float = 0.1, test_ratio: float = 0.1,
                  seed: int = 42) -> Tuple[List[Dict[str, Any]],
List[Dict[str, Any]],
List[Dict[str, Any]]]:
    """
    åˆ†å‰²æ•°æ®é›†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†

    Args:
        dataset: æ•°æ®é›†
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
        test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
        seed: éšæœºç§å­

    Returns:
        åˆ†å‰²åçš„è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†
    """
    if train_ratio + val_ratio + test_ratio != 1.0:
        raise ValueError("Ratios must sum to 1.0")

    # è®¾ç½®éšæœºç§å­
    np.random.seed(seed)
    random.seed(seed)

    # æ‰“ä¹±æ•°æ®é›†
    indices = list(range(len(dataset)))
    random.shuffle(indices)

    # è®¡ç®—åˆ†å‰²ç‚¹
    train_size = int(len(dataset) * train_ratio)
    val_size = int(len(dataset) * val_ratio)

    # åˆ†å‰²æ•°æ®é›†
    train_indices = indices[:train_size]
    val_indices = indices[train_size:train_size + val_size]
    test_indices = indices[train_size + val_size:]

    # åˆ›å»ºåˆ†å‰²åçš„æ•°æ®é›†
    train_data = [dataset[i] for i in train_indices]
    val_data = [dataset[i] for i in val_indices]
    test_data = [dataset[i] for i in test_indices]

    return train_data, val_data, test_data


# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    # ä»transformersåŠ è½½åˆ†è¯å™¨
    from transformers import AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")

    # åˆ›å»ºé¢„å¤„ç†å™¨
    preprocessor = Web3DataPreprocessor(
        tokenizer=tokenizer,
        language='chinese',
        lower_case=True,
        remove_stopwords=True,
        normalize_web3_terms=True
    )

    # ç¤ºä¾‹æ–‡æœ¬
    sample_text = "ğŸ“¢ã€é‡è¦é€šçŸ¥ã€‘ä»¥å¤ªåŠLayer 2è§£å†³æ–¹æ¡ˆå°†é™ä½Gasè´¹å¹¶æé«˜äº¤æ˜“é€Ÿåº¦ï¼" + \
                  "å½“å‰ETHä»·æ ¼ä¸º$1,800ï¼Œè¾ƒæ˜¨æ—¥ä¸Šæ¶¨2.5%ã€‚0x1234...abcdæ˜¯ä¸€ä¸ªæµ‹è¯•åœ°å€ã€‚"

    # å¤„ç†å•ä¸ªæ–‡æœ¬
    processed = preprocessor.process_text(sample_text)

    print("åŸå§‹æ–‡æœ¬:", sample_text)
    print("\næ¸…æ´—åçš„æ–‡æœ¬:", processed["cleaned_text"])
    print("\næ ‡å‡†åŒ–åçš„æ–‡æœ¬:", processed["normalized_text"])
    print("\nåˆ†è¯ç»“æœ:", processed["tokens"])
    print("\næ•°å€¼ç‰¹å¾:", {k: v for k, v in processed.items() if k.startswith("numeric_")})
    print("\nWeb3ç‰¹å¾:", {k: v for k, v in processed.items() if k.startswith("web3_")})

    # å¦‚æœæœ‰åˆ†è¯å™¨ï¼Œæ˜¾ç¤ºç¼–ç ç»“æœ
    if "input_ids" in processed:
        print("\nç¼–ç åçš„è¾“å…¥ID:", processed["input_ids"][:10], "...")
        print("æ³¨æ„åŠ›æ©ç :", processed["attention_mask"][:10], "...")

    # ç¤ºä¾‹æ•°æ®é›†
    sample_dataset = [
        {
            "id": 1,
            "text": "å¦‚ä½•åœ¨ä»¥å¤ªåŠä¸Šéƒ¨ç½²æ™ºèƒ½åˆçº¦ï¼Ÿ",
            "label": 0
        },
        {
            "id": 2,
            "text": "æ¯”ç‰¹å¸ä»·æ ¼é¢„æµ‹ï¼šæœªæ¥ä¸€å‘¨BTCä¼šç»§ç»­ä¸Šæ¶¨å—ï¼Ÿ",
            "label": 1
        },
        {
            "id": 3,
            "text": "è¿™æ˜¯ä¸€ä¸ªå…³äºDAOæ²»ç†çš„è®¨è®ºï¼Œæ¬¢è¿å‚ä¸ï¼",
            "label": 0
        }
    ]

    # åˆ›å»ºæ•°æ®é›†å¤„ç†å™¨
    dataset_processor = Web3DatasetProcessor(preprocessor)

    # å¤„ç†æ•°æ®é›†
    processed_dataset = dataset_processor.process_dataset(sample_dataset)

    print("\nå¤„ç†åçš„æ•°æ®é›†:")
    for item in processed_dataset:
        print(f"ID: {item['id']}, æ ‡ç­¾: {item.get('label')}")
        print(f"æ¸…æ´—åçš„æ–‡æœ¬: {item['cleaned_text']}")
        print(f"Web3æœ¯è¯­æ€»æ•°: {item['web3_term_total']}")
        print("-" * 40)

    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(sample_dataset)
    processed_df = dataset_processor.process_dataframe(df)

    print("\nå¤„ç†åçš„DataFrame:")
    print(processed_df[["id", "label", "cleaned_text", "web3_term_total"]])

    # åˆ›å»ºPyTorchæ•°æ®é›†
    torch_dataset = dataset_processor.create_torch_dataset(processed_dataset)

    print("\nPyTorchæ•°æ®é›†æ ·æœ¬:")
    sample = torch_dataset[0]
    for key, value in sample.items():
        print(f"{key}: {value.shape if hasattr(value, 'shape') else value[:10]}")