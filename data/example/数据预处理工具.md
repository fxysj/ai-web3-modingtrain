æ•°æ®é¢„å¤„ç†å·¥å…·ç‰¹ç‚¹
è¿™ä¸ªæ•°æ®é¢„å¤„ç†å·¥å…·ä¸“ä¸º Web3.0 é¢†åŸŸè®¾è®¡ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š
é¢†åŸŸç‰¹å®šå¤„ç†ï¼šå†…ç½® Web3.0 é¢†åŸŸæœ¯è¯­è¡¨ï¼Œèƒ½å¤Ÿè¯†åˆ«å’Œæ ‡å‡†åŒ–åŒºå—é“¾ã€åŠ å¯†è´§å¸ç›¸å…³æœ¯è¯­
å…¨é¢çš„æ–‡æœ¬æ¸…æ´—ï¼šèƒ½å¤Ÿå¤„ç† URLã€HTML æ ‡ç­¾ã€ç”µå­é‚®ä»¶ã€ä»¥å¤ªåŠåœ°å€ç­‰ç‰¹æ®Šå†…å®¹
è¡¨æƒ…ç¬¦å·å¤„ç†ï¼šå°†è¡¨æƒ…ç¬¦å·è½¬æ¢ä¸ºæ–‡æœ¬æè¿°ï¼Œä¿ç•™æƒ…æ„Ÿä¿¡æ¯
æ•°å€¼ç‰¹å¾æå–ï¼šä¸“é—¨å¤„ç†ä»·æ ¼ã€ç™¾åˆ†æ¯”ç­‰ Web3.0 é¢†åŸŸå¸¸è§çš„æ•°å€¼æ•°æ®
ç‰¹å¾å·¥ç¨‹ï¼šæå– Web3.0 ç‰¹å®šç‰¹å¾ï¼Œå¦‚åè®®åç§°ã€æœ¯è¯­é¢‘ç‡ç­‰
çµæ´»çš„è¾“å‡ºæ ¼å¼ï¼šæ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼ï¼ŒåŒ…æ‹¬å­—å…¸åˆ—è¡¨ã€DataFrame å’Œ PyTorch æ•°æ®é›†


ä½¿ç”¨ç¤ºä¾‹
1. å•æ–‡æœ¬é¢„å¤„ç†
from utils.data_preprocessing import Web3DataPreprocessor
from transformers import AutoTokenizer

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")

# åˆ›å»ºé¢„å¤„ç†å™¨
preprocessor = Web3DataPreprocessor(
    tokenizer=tokenizer,
    language='chinese',
    lower_case=True,
    remove_stopwords=True,
    normalize_web3_terms=True
)

# ç¤ºä¾‹æ–‡æœ¬
text = "ğŸ“¢ã€é‡è¦é€šçŸ¥ã€‘ä»¥å¤ªåŠLayer 2è§£å†³æ–¹æ¡ˆå°†é™ä½Gasè´¹å¹¶æé«˜äº¤æ˜“é€Ÿåº¦ï¼" + \
       "å½“å‰ETHä»·æ ¼ä¸º$1,800ï¼Œè¾ƒæ˜¨æ—¥ä¸Šæ¶¨2.5%ã€‚0x1234...abcdæ˜¯ä¸€ä¸ªæµ‹è¯•åœ°å€ã€‚"

# å¤„ç†æ–‡æœ¬
processed = preprocessor.process_text(text)

print("åŸå§‹æ–‡æœ¬:", text)
print("\næ¸…æ´—åçš„æ–‡æœ¬:", processed["cleaned_text"])
print("\næ ‡å‡†åŒ–åçš„æ–‡æœ¬:", processed["normalized_text"])
print("\nåˆ†è¯ç»“æœ:", processed["tokens"])
print("\næ•°å€¼ç‰¹å¾:", {k: v for k, v in processed.items() if k.startswith("numeric_")})
print("\nWeb3ç‰¹å¾:", {k: v for k, v in processed.items() if k.startswith("web3_")})

2. å¤„ç†æ•°æ®é›†
from utils.data_preprocessing import Web3DataPreprocessor, Web3DatasetProcessor
from transformers import AutoTokenizer

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")

# åˆ›å»ºé¢„å¤„ç†å™¨
preprocessor = Web3DataPreprocessor(tokenizer=tokenizer)

# åˆ›å»ºæ•°æ®é›†å¤„ç†å™¨
dataset_processor = Web3DatasetProcessor(preprocessor)

# ç¤ºä¾‹æ•°æ®é›†
dataset = [
    {
        "id": 1,
        "text": "å¦‚ä½•åœ¨ä»¥å¤ªåŠä¸Šéƒ¨ç½²æ™ºèƒ½åˆçº¦ï¼Ÿ",
        "label": 0
    },
    {
        "id": 2,
        "text": "æ¯”ç‰¹å¸ä»·æ ¼é¢„æµ‹ï¼šæœªæ¥ä¸€å‘¨BTCä¼šç»§ç»­ä¸Šæ¶¨å—ï¼Ÿ",
        "label": 1
    }
]

# å¤„ç†æ•°æ®é›†
processed_dataset = dataset_processor.process_dataset(dataset)

print("å¤„ç†åçš„æ•°æ®é›†:")
for item in processed_dataset[:2]:
    print(f"ID: {item['id']}, æ ‡ç­¾: {item.get('label')}")
    print(f"æ¸…æ´—åçš„æ–‡æœ¬: {item['cleaned_text']}")
    print(f"Web3æœ¯è¯­: {item['web3_term_counts']}")
    print("-" * 40)

3. è½¬æ¢ä¸º PyTorch æ•°æ®é›†
from utils.data_preprocessing import Web3DataPreprocessor, Web3DatasetProcessor
from transformers import AutoTokenizer
from torch.utils.data import DataLoader

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")

# åˆ›å»ºé¢„å¤„ç†å™¨
preprocessor = Web3DataPreprocessor(tokenizer=tokenizer)

# åˆ›å»ºæ•°æ®é›†å¤„ç†å™¨
dataset_processor = Web3DatasetProcessor(preprocessor)

# ç¤ºä¾‹æ•°æ®é›†
dataset = [
    {"text": "ä»¥å¤ªåŠæ˜¯ä¸€ä¸ªå¼€æºçš„åŒºå—é“¾å¹³å°", "label": 0},
    {"text": "æ¯”ç‰¹å¸æ˜¯ç¬¬ä¸€ç§å»ä¸­å¿ƒåŒ–æ•°å­—è´§å¸", "label": 1}
]

# å¤„ç†æ•°æ®é›†
processed_dataset = dataset_processor.process_dataset(dataset)

# åˆ›å»ºPyTorchæ•°æ®é›†
torch_dataset = dataset_processor.create_torch_dataset(processed_dataset)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
dataloader = DataLoader(torch_dataset, batch_size=2)

# æµ‹è¯•æ•°æ®åŠ è½½å™¨
batch = next(iter(dataloader))
print("æ‰¹æ¬¡æ•°æ®:")
for key, value in batch.items():
    print(f"{key}: {value.shape}")

è¿™ä¸ªæ•°æ®é¢„å¤„ç†å·¥å…·å¯ä»¥å¸®åŠ©ä½ åœ¨è®­ç»ƒ Web3.0 ç›¸å…³æ¨¡å‹æ—¶å‡†å¤‡é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œæé«˜æ¨¡å‹çš„æ€§èƒ½å’Œå¯¹é¢†åŸŸæœ¯è¯­çš„ç†è§£èƒ½åŠ›ã€‚