功能说明
模型加载：
支持 Hugging Face 原生模型和 vLLM 高性能推理引擎
自动处理设备分配（CPU/GPU）和混合精度
输入输出：
支持 JSONL 格式输入文件（每行包含query字段）
保存结果为 JSONL 格式，包含生成参数和时间戳
自动保存示例结果以便人工检查
生成控制：
支持温度、核采样（top_p）、束搜索等生成参数
可自定义最大生成长度，适配 Web3.0 长文本生成需求
性能优化：
vLLM 支持多 GPU 并行推理，大幅提升吞吐量
批量处理减少推理延迟，适合大规模查询场景
结果评估：
可选计算 ROUGE 和 BLEU 指标（需提供参考标准答案）
生成详细的评估报告，便于模型效果分析


使用示例
1. 基础推理（使用 vLLM 加速）

python scripts/run_inference.py \
  --model_path ./web3_finetuned_model \
  --input_file web3_queries.jsonl \
  --output_file results.jsonl \
  --use_vllm \
  --batch_size 16


2. 带评估的推理（需提供参考文件）

python scripts/run_inference.py \
  --model_path ./web3_finetuned_model \
  --input_file web3_queries.jsonl \
  --output_file results.jsonl \
  --reference_file web3_answers.jsonl \
  --temperature 0.5 \
  --num_beams 8

3. CPU 环境推理（测试或小规模场景）

python scripts/run_inference.py \
  --model_path ./web3_finetuned_model \
  --input_file web3_queries.jsonl \
  --output_file results.jsonl \
  --device cpu

扩展建议
流式输出：
添加--stream参数，支持逐 Token 流式生成，适用于实时交互场景
自定义模板：
通过--prompt_template参数加载自定义提示模板，适配不同 Web3.0 任务（如交易分析、智能合约审计）
缓存机制：
增加结果缓存功能，避免重复查询相同问题，提升响应速度
量化支持：
集成 GGML/QLoRA 等量化方案，支持在低资源设备上运行大模型
这个脚本提供了生产环境可用的推理能力，可直接用于 Web3.0 领域的链上数据分析、市场预测、用户行为建模等场景


